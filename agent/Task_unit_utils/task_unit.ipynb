{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00016-of-00027-89dd493e8abfbe97.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00026-of-00027-fdc31d0e2d56bf2f.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00005-of-00027-5880578c1eba7822.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00010-of-00027-dc9d9f0049e1567c.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00002-of-00027-81107b64e8a3a046.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00001-of-00027-2011d8e72a165f62.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00008-of-00027-a361767e10599c01.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00020-of-00027-a3f17abfa6315328.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00007-of-00027-ae4267e946757225.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00018-of-00027-59871098159f2152.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00009-of-00027-6389d25433f33aeb.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00003-of-00027-b2bcc4d20fbfb47d.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00000-of-00027-4d11798d7219186d.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00014-of-00027-ed3e2dfdf0245d74.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00012-of-00027-e10fbc31995f4638.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00013-of-00027-f12808857fc81595.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00024-of-00027-c75dad7737630204.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00017-of-00027-2c7d6427a32411c4.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00019-of-00027-55293ecc88d419ef.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00021-of-00027-52891bb870366cec.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00023-of-00027-9cb87a52ffbec4a9.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00025-of-00027-903fd897f9bfb606.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00006-of-00027-94de4ed04ae0b588.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00004-of-00027-82ef57959581c455.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00011-of-00027-d307a333ead09969.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00015-of-00027-3e1bf87116e5d714.parquet', '/home/douzc/users/yuyao/datasets/Mind2Web/data/train-00022-of-00027-12501ae3fc70271f.parquet']\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28mprint\u001b[39m(train_files)\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# 读取并合并所有 train 文件\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m df_train = pd.concat([\u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m train_files], ignore_index=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     20\u001b[39m df = df_train.drop_duplicates(subset=[\u001b[33m\"\u001b[39m\u001b[33mannotation_id\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m     22\u001b[39m \u001b[38;5;66;03m# output_dir = \"dataset\"\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# file_name = \"mind2web_dataset.joblib\"\u001b[39;00m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# save_path = os.path.join(output_dir, file_name)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     35\u001b[39m \u001b[38;5;66;03m# except Exception as e:\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m#     print(f\"保存 DataFrame 时出错: {e}\")\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/pandas/io/parquet.py:669\u001b[39m, in \u001b[36mread_parquet\u001b[39m\u001b[34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[39m\n\u001b[32m    666\u001b[39m     use_nullable_dtypes = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    667\u001b[39m check_dtype_backend(dtype_backend)\n\u001b[32m--> \u001b[39m\u001b[32m669\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimpl\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    670\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    671\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    672\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    673\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    674\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_nullable_dtypes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    675\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    676\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfilesystem\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    677\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    678\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/pandas/io/parquet.py:279\u001b[39m, in \u001b[36mPyArrowImpl.read\u001b[39m\u001b[34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m catch_warnings():\n\u001b[32m    274\u001b[39m     filterwarnings(\n\u001b[32m    275\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    276\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmake_block is deprecated\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    277\u001b[39m         \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m,\n\u001b[32m    278\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m279\u001b[39m     result = \u001b[43marrow_table_to_pandas\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    282\u001b[39m \u001b[43m        \u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m manager == \u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    286\u001b[39m     result = result._as_manager(\u001b[33m\"\u001b[39m\u001b[33marray\u001b[39m\u001b[33m\"\u001b[39m, copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/pandas/io/_util.py:93\u001b[39m, in \u001b[36marrow_table_to_pandas\u001b[39m\u001b[34m(table, dtype_backend, null_to_int64, to_pandas_kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     91\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m df = \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_pandas\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtypes_mapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtypes_mapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mto_pandas_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/pyarrow/array.pxi:987\u001b[39m, in \u001b[36mpyarrow.lib._PandasConvertible.to_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/pyarrow/table.pxi:5174\u001b[39m, in \u001b[36mpyarrow.lib.Table._to_pandas\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/pyarrow/pandas_compat.py:808\u001b[39m, in \u001b[36mtable_to_dataframe\u001b[39m\u001b[34m(options, table, categories, ignore_metadata, types_mapper)\u001b[39m\n\u001b[32m    805\u001b[39m columns = _deserialize_column_index(table, all_columns, column_indexes)\n\u001b[32m    807\u001b[39m column_names = table.column_names\n\u001b[32m--> \u001b[39m\u001b[32m808\u001b[39m result = \u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable_to_blocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcategories\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    809\u001b[39m \u001b[43m                                \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mext_columns_dtypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _pandas_api.is_ge_v3():\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minternals\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_dataframe_from_blocks\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "import pandas as pd\n",
    "import os\n",
    "from glob import glob\n",
    "import prompt\n",
    "import joblib\n",
    "\n",
    "\n",
    "# ds_test_task = pd.DataFrame(load_dataset(\"osunlp/Multimodal-Mind2Web\", split = \"test_task\"))\n",
    "# ds_test_website = pd.DataFrame(load_dataset(\"osunlp/Multimodal-Mind2Web\", split = \"test_website\"))\n",
    "# ds_test_domain = pd.DataFrame(load_dataset(\"osunlp/Multimodal-Mind2Web\", split = \"test_domain\"))\n",
    "# 指定数据目录\n",
    "data_dir = '/home/douzc/users/yuyao/datasets/Mind2Web/data'\n",
    "\n",
    "# 获取所有 train 文件的路径\n",
    "train_files = glob(os.path.join(data_dir, 'train-*.parquet'))\n",
    "print(train_files)\n",
    "# 读取并合并所有 train 文件\n",
    "df_train = pd.concat([pd.read_parquet(file) for file in train_files], ignore_index=True)\n",
    "df = df_train.drop_duplicates(subset=[\"annotation_id\"])\n",
    "\n",
    "# output_dir = \"dataset\"\n",
    "# file_name = \"mind2web_dataset.joblib\"\n",
    "# save_path = os.path.join(output_dir, file_name)\n",
    "\n",
    "# # 确保输出目录存在\n",
    "# os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# print(f\"开始保存 DataFrame 到 {save_path}...\")\n",
    "# try:\n",
    "#     # 使用 joblib 保存，compress=3 表示使用 zlib 压缩等级 3 (平衡速度和压缩率)\n",
    "#     # 您可以调整 compress 的值 (0-9)，0 表示不压缩，9 表示最大压缩\n",
    "#     joblib.dump(df, save_path, compress=3)\n",
    "#     print(f\"DataFrame 成功保存到 {save_path}\")\n",
    "# except Exception as e:\n",
    "#     print(f\"保存 DataFrame 时出错: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 16:09:08,542 - faiss.loader - INFO - Loading faiss with AVX512 support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-21 16:09:08,633 - faiss.loader - INFO - Successfully loaded faiss with AVX512 support.\n",
      "2025-06-21 16:09:08,638 - faiss - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "2025-06-21 16:09:09,635 - EmbeddingManager - INFO - OpenAI 客户端初始化成功。\n",
      "2025-06-21 16:09:09,636 - EmbeddingManager - INFO - 尝试加载所有可用的中间结果...\n",
      "2025-06-21 16:09:09,637 - EmbeddingManager - INFO - 加载嵌入相关数据...\n",
      "2025-06-21 16:09:09,645 - EmbeddingManager - ERROR - 加载嵌入相关数据时出错: Failed to read all data for array. Expected (1009, 1536) = 1549824 elements, could only read 1441760 elements. (file seems not fully written?)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/douzc/users/yuyao/browser_use_taskunit/agent/Task_unit_utils/EmbeddingManager.py\", line 354, in _load_embeddings_data\n",
      "    loaded_embeddings = np.load(self.embeddings_file)\n",
      "                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/numpy/lib/_npyio_impl.py\", line 483, in load\n",
      "    return format.read_array(fid, allow_pickle=allow_pickle,\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/data00/yangzhao/anaconda3/envs/zyy/lib/python3.12/site-packages/numpy/lib/_format_impl.py\", line 874, in read_array\n",
      "    raise ValueError(\n",
      "ValueError: Failed to read all data for array. Expected (1009, 1536) = 1549824 elements, could only read 1441760 elements. (file seems not fully written?)\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import prompt\n",
    "import logging\n",
    "import sys\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,  # 设置日志级别为 INFO，这样 INFO 及以上级别的日志都会被处理\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s', # 定义日志格式\n",
    "    stream=sys.stdout  # 将日志输出到标准输出 (控制台)，默认为 sys.stderr\n",
    "    # 如果不写 stream=sys.stdout，日志会输出到标准错误流，通常也在控制台显示，但有时会有不同处理\n",
    ")\n",
    "\n",
    "importlib.reload(prompt)\n",
    "# 初始化服务管理器\n",
    "service_manager = prompt.AIServiceManager(config_path=\"api_config.json\")\n",
    "\n",
    "\n",
    "from EmbeddingManager import TaskEmbeddingManager\n",
    "embedding_manager = TaskEmbeddingManager(service_manager=service_manager, dataset_path=\"./dataset/mind2web_dataset.joblib\",openai_api_key=\"sk-VBtOoMbXC0U5ESUL9d2c39979bE548Cd9cDa8fE1F2264c83\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results = await prompt.run_cluster_task_units(clusters_by_distance, df, mode=\"formal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import graph\n",
    "# await graph.main_cluster_async(\n",
    "#         clusters_by_distance=clusters_by_distance,\n",
    "#         df=df,\n",
    "#         max_concurrency=12,\n",
    "#         mode=\"formal\"\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 19:35:08,434 - EmbeddingManager - INFO - \n",
      "开始执行层次聚类...\n",
      "2025-04-20 19:35:08,436 - EmbeddingManager - INFO - 计算链接矩阵 (方法: ward, 度量: euclidean) for 1009 vectors...\n",
      "2025-04-20 19:35:08,780 - EmbeddingManager - INFO - 链接矩阵计算完成。\n",
      "2025-04-20 19:35:08,781 - EmbeddingManager - INFO - 根据标准 'distance' 和阈值 0.9 获取扁平聚类结果...\n",
      "2025-04-20 19:35:08,784 - EmbeddingManager - INFO - 聚类完成，得到 29 个簇。\n",
      "2025-04-20 19:35:08,785 - EmbeddingManager - INFO - 开始计算簇质心...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "计算质心: 100%|██████████| 29/29 [00:00<00:00, 11356.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 19:35:08,791 - EmbeddingManager - INFO - 质心计算完成，共计算了 29 个簇的质心。\n",
      "2025-04-20 19:35:08,792 - EmbeddingManager - WARNING - FAISS 反向映射不可用，无法映射聚类标签到 DataFrame。\n",
      "2025-04-20 19:35:08,793 - EmbeddingManager - INFO - 保存簇质心...\n",
      "2025-04-20 19:35:08,796 - EmbeddingManager - INFO - 成功保存簇质心。\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([25, 15,  1, ..., 26, 21, 19], dtype=int32)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_manager.perform_hierarchical_clustering()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-04-20 19:46:39,390 - EmbeddingManager - INFO - 开始搜索与 'Buy a copy of the Gorillaz first studio album....' 相关的 task units (初始 k=5, max_units=50)...\n",
      "2025-04-20 19:46:39,392 - EmbeddingManager - INFO - 尝试使用 k=5 搜索簇...\n",
      "2025-04-20 19:46:48,065 - httpx - INFO - HTTP Request: POST https://aihubmix.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-20 19:46:48,072 - EmbeddingManager - INFO - 对于查询 'Buy a copy of the Gorillaz first studio album....' 找到最近的 5 个簇标签: [9, 15, 7, 3, 14]\n",
      "2025-04-20 19:46:48,072 - EmbeddingManager - INFO - 使用 k=5 找到 5 个相关簇: [9, 15, 7, 3, 14]\n",
      "2025-04-20 19:46:48,074 - EmbeddingManager - INFO - 使用 k=5 加载了 97 个 Task Units。\n",
      "2025-04-20 19:46:48,075 - EmbeddingManager - WARNING - Task units 数量 (97) 超过限制 (50)，尝试减小 k。\n",
      "2025-04-20 19:46:48,075 - EmbeddingManager - INFO - 尝试使用 k=4 搜索簇...\n",
      "2025-04-20 19:46:55,611 - httpx - INFO - HTTP Request: POST https://aihubmix.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-20 19:46:55,615 - EmbeddingManager - INFO - 对于查询 'Buy a copy of the Gorillaz first studio album....' 找到最近的 4 个簇标签: [9, 7, 15, 3]\n",
      "2025-04-20 19:46:55,616 - EmbeddingManager - INFO - 使用 k=4 找到 4 个相关簇: [9, 7, 15, 3]\n",
      "2025-04-20 19:46:55,619 - EmbeddingManager - INFO - 使用 k=4 加载了 88 个 Task Units。\n",
      "2025-04-20 19:46:55,619 - EmbeddingManager - WARNING - Task units 数量 (88) 超过限制 (50)，尝试减小 k。\n",
      "2025-04-20 19:46:55,620 - EmbeddingManager - INFO - 尝试使用 k=3 搜索簇...\n",
      "2025-04-20 19:47:03,968 - httpx - INFO - HTTP Request: POST https://aihubmix.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-20 19:47:03,973 - EmbeddingManager - INFO - 对于查询 'Buy a copy of the Gorillaz first studio album....' 找到最近的 3 个簇标签: [9, 15, 7]\n",
      "2025-04-20 19:47:03,973 - EmbeddingManager - INFO - 使用 k=3 找到 3 个相关簇: [9, 15, 7]\n",
      "2025-04-20 19:47:03,976 - EmbeddingManager - INFO - 使用 k=3 加载了 82 个 Task Units。\n",
      "2025-04-20 19:47:03,976 - EmbeddingManager - WARNING - Task units 数量 (82) 超过限制 (50)，尝试减小 k。\n",
      "2025-04-20 19:47:03,977 - EmbeddingManager - INFO - 尝试使用 k=2 搜索簇...\n",
      "2025-04-20 19:47:11,895 - httpx - INFO - HTTP Request: POST https://aihubmix.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-20 19:47:11,899 - EmbeddingManager - INFO - 对于查询 'Buy a copy of the Gorillaz first studio album....' 找到最近的 2 个簇标签: [9, 15]\n",
      "2025-04-20 19:47:11,900 - EmbeddingManager - INFO - 使用 k=2 找到 2 个相关簇: [9, 15]\n",
      "2025-04-20 19:47:11,902 - EmbeddingManager - INFO - 使用 k=2 加载了 72 个 Task Units。\n",
      "2025-04-20 19:47:11,903 - EmbeddingManager - WARNING - Task units 数量 (72) 超过限制 (50)，尝试减小 k。\n",
      "2025-04-20 19:47:11,904 - EmbeddingManager - INFO - 尝试使用 k=1 搜索簇...\n",
      "2025-04-20 19:47:20,472 - httpx - INFO - HTTP Request: POST https://aihubmix.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "2025-04-20 19:47:20,476 - EmbeddingManager - INFO - 对于查询 'Buy a copy of the Gorillaz first studio album....' 找到最近的 1 个簇标签: [9]\n",
      "2025-04-20 19:47:20,477 - EmbeddingManager - INFO - 使用 k=1 找到 1 个相关簇: [9]\n",
      "2025-04-20 19:47:20,478 - EmbeddingManager - INFO - 使用 k=1 加载了 10 个 Task Units。\n",
      "2025-04-20 19:47:20,479 - EmbeddingManager - INFO - Task units 数量 (10) 在限制 (50) 内，使用当前 k=1。\n",
      "2025-04-20 19:47:20,480 - EmbeddingManager - INFO - 最终使用 10 个 Task Units (来自 k=1 个簇) 构建 prompt。\n",
      "2025-04-20 19:47:20,481 - EmbeddingManager - INFO - 正在调用模型分析相关 task units...\n",
      "{\n",
      "  \"relevant_task_units\": [\n",
      "    {\n",
      "      \"id\": \"CLUSTER9_TU1\",\n",
      "      \"name\": \"Search for and select a specific music album\",\n",
      "      \"sub_actions\": [\n",
      "        \"TYPE: Artist or album name\",\n",
      "        \"CLICK: Search result matching the artist or album\",\n",
      "        \"CLICK: Option to purchase or view details\"\n",
      "      ],\n",
      "      \"reason\": \"This task unit is directly relevant as it involves searching for and selecting the Gorillaz first studio album, which is the core action needed to accomplish the task.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"CLUSTER9_TU2\",\n",
      "      \"name\": \"Filter and select music items based on criteria\",\n",
      "      \"sub_actions\": [\n",
      "        \"CLICK: Marketplace or explore section\",\n",
      "        \"SELECT: Genre\",\n",
      "        \"SELECT: Country\",\n",
      "        \"SELECT: Format\",\n",
      "        \"SELECT: Release year\",\n",
      "        \"SELECT: Price range\",\n",
      "        \"SELECT: Condition\",\n",
      "        \"CLICK: Item matching criteria\",\n",
      "        \"CLICK: Add to cart or purchase option\"\n",
      "      ],\n",
      "      \"reason\": \"This task unit may be relevant if the user needs to filter the search results to find the specific album based on criteria like format (e.g., CD, vinyl) or condition (e.g., new, used).\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "task_1=\"Buy a copy of the Gorillaz first studio album.\"\n",
    "result = embedding_manager.search_nearest_taskunit(task_1)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zyy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
